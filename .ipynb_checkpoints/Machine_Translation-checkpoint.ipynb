{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7BGYRK2eF5G0",
    "outputId": "67384f6f-1a92-4b06-f2c2-29b95f6ea6dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n",
      "1.9.0\n"
     ]
    }
   ],
   "source": [
    "# Uncomment the following if need GPU\n",
    "# import tensorflow as tf\n",
    "# device_name = tf.test.gpu_device_name()\n",
    "# if device_name != '/device:GPU:0':\n",
    "#   raise SystemError('GPU device not found')\n",
    "# print('Found GPU at: {}'.format(device_name))\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "print(torch.__version__)\n",
    "\n",
    "# keras\n",
    "import keras\n",
    "from keras.layers import Input, Dense, Conv2D, Dropout, Activation, Flatten, Reshape, Softmax, MaxPooling2D\n",
    "from keras.models import Model, Sequential, clone_model\n",
    "\n",
    "# scikit-learn\n",
    "import sklearn\n",
    "from sklearn import cluster, decomposition, manifold, metrics\n",
    "import pandas as pd\n",
    "\n",
    "import scipy\n",
    "from scipy.stats import entropy\n",
    "from scipy.stats import dirichlet\n",
    "\n",
    "from os.path import exists\n",
    "# access google file system\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "cwd = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQtYy9NGGpRR"
   },
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YDhXD9yBGZKX"
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "class Example(object):\n",
    "    \"\"\"\n",
    "    Wrapper class for a single (natural language, logical form) input/output (x/y) pair\n",
    "    Attributes:\n",
    "        x: the natural language as one string\n",
    "        x_tok: tokenized natural language as a list of strings\n",
    "        x_indexed: indexed tokens, a list of ints\n",
    "        y: the raw logical form as a string\n",
    "        y_tok: tokenized logical form, a list of strings\n",
    "        y_indexed: indexed logical form, a list of ints\n",
    "    \"\"\"\n",
    "    def __init__(self, x: str, x_tok: List[str], x_indexed: List[int], y, y_tok, y_indexed):\n",
    "        self.x = x\n",
    "        self.x_tok = x_tok\n",
    "        self.x_indexed = x_indexed\n",
    "        self.y = y\n",
    "        self.y_tok = y_tok\n",
    "        self.y_indexed = y_indexed\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \" \".join(self.x_tok) + \" => \" + \" \".join(self.y_tok) + \"\\n   indexed as: \" + repr(self.x_indexed) + \" => \" + repr(self.y_indexed)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "class Derivation(object):\n",
    "    \"\"\"\n",
    "    Wrapper for a possible solution returned by the model associated with an Example. Note that y_toks here is a\n",
    "    predicted y_toks, and the Example itself contains the gold y_toks.\n",
    "    Attributes:\n",
    "          example: The underlying Example we're predicting on\n",
    "          p: the probabilities associated with this prediction\n",
    "          y_toks: the tokenized output prediction\n",
    "    \"\"\"\n",
    "    def __init__(self, example: Example, p, y_toks):\n",
    "        self.example = example\n",
    "        self.p = p\n",
    "        self.y_toks = y_toks\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"%s (%s)\" % (self.y_toks, self.p)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "\n",
    "PAD_SYMBOL = \"<PAD>\"\n",
    "UNK_SYMBOL = \"<UNK>\"\n",
    "SOS_SYMBOL = \"<SOS>\"\n",
    "EOS_SYMBOL = \"<EOS>\"\n",
    "\n",
    "class Indexer(object):\n",
    "    \"\"\"\n",
    "    Bijection between objects and integers starting at 0. Useful for mapping\n",
    "    labels, features, etc. into coordinates of a vector space.\n",
    "\n",
    "    Attributes:\n",
    "        objs_to_ints\n",
    "        ints_to_objs\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.objs_to_ints = {}\n",
    "        self.ints_to_objs = {}\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str([str(self.get_object(i)) for i in range(0, len(self))])\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.objs_to_ints)\n",
    "\n",
    "    def get_object(self, index):\n",
    "        \"\"\"\n",
    "        :param index: integer index to look up\n",
    "        :return: Returns the object corresponding to the particular index or None if not found\n",
    "        \"\"\"\n",
    "        if (index not in self.ints_to_objs):\n",
    "            return None\n",
    "        else:\n",
    "            return self.ints_to_objs[index]\n",
    "\n",
    "    def contains(self, object):\n",
    "        \"\"\"\n",
    "        :param object: object to look up\n",
    "        :return: Returns True if it is in the Indexer, False otherwise\n",
    "        \"\"\"\n",
    "        return self.index_of(object) != -1\n",
    "\n",
    "    def index_of(self, object):\n",
    "        \"\"\"\n",
    "        :param object: object to look up\n",
    "        :return: Returns -1 if the object isn't present, index otherwise\n",
    "        \"\"\"\n",
    "        if (object not in self.objs_to_ints):\n",
    "            return -1\n",
    "        else:\n",
    "            return self.objs_to_ints[object]\n",
    "\n",
    "    def add_and_get_index(self, object, add=True):\n",
    "        \"\"\"\n",
    "        Adds the object to the index if it isn't present, always returns a nonnegative index\n",
    "        :param object: object to look up or add\n",
    "        :param add: True by default, False if we shouldn't add the object. If False, equivalent to index_of.\n",
    "        :return: The index of the object\n",
    "        \"\"\"\n",
    "        if not add:\n",
    "            return self.index_of(object)\n",
    "        if (object not in self.objs_to_ints):\n",
    "            new_idx = len(self.objs_to_ints)\n",
    "            self.objs_to_ints[object] = new_idx\n",
    "            self.ints_to_objs[new_idx] = object\n",
    "        return self.objs_to_ints[object]\n",
    "\n",
    "def make_padded_input_tensor(exs: List[Example], input_indexer: Indexer, max_len: int, reverse_input=False) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Takes the given Examples and their input indexer and turns them into a numpy array by padding them out to max_len.\n",
    "    Optionally reverses them.\n",
    "    :param exs: examples to tensor-ify\n",
    "    :param input_indexer: Indexer over input symbols; needed to get the index of the pad symbol\n",
    "    :param max_len: max input len to use (pad/truncate to this length)\n",
    "    :param reverse_input: True if we should reverse the inputs (useful if doing a unidirectional LSTM encoder)\n",
    "    :return: A [num example, max_len]-size array of indices of the input tokens\n",
    "    \"\"\"\n",
    "    if reverse_input:\n",
    "        return np.array(\n",
    "            [[ex.x_indexed[len(ex.x_indexed) - 1 - i] if i < len(ex.x_indexed) else input_indexer.index_of(PAD_SYMBOL)\n",
    "              for i in range(0, max_len)]\n",
    "             for ex in exs])\n",
    "    else:\n",
    "        return np.array([[ex.x_indexed[i] if i < len(ex.x_indexed) else input_indexer.index_of(PAD_SYMBOL)\n",
    "                          for i in range(0, max_len)]\n",
    "                         for ex in exs])\n",
    "\n",
    "def make_padded_output_tensor(exs, output_indexer, max_len):\n",
    "    \"\"\"\n",
    "    Similar to make_padded_input_tensor, but does it on the outputs without the option to reverse input\n",
    "    :param exs:\n",
    "    :param output_indexer:\n",
    "    :param max_len:\n",
    "    :return: A [num example, max_len]-size array of indices of the output tokens\n",
    "    \"\"\"\n",
    "    return np.array([[ex.y_indexed[i] if i < len(ex.y_indexed) else output_indexer.index_of(PAD_SYMBOL) for i in range(0, max_len)] for ex in exs])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRt_saWrHdJ_"
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FaEIsUvTGupr"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def load_datasets(train_paths:Tuple[str,str], dev_paths:Tuple[str,str]) -> (List[Tuple[str,str]], List[Tuple[str,str]]):\n",
    "    \"\"\"\n",
    "    Reads the training, dev, and test data from the corresponding files.\n",
    "    :param train_path:\n",
    "    :param dev_path:\n",
    "    :param test_path:\n",
    "    :param domain: Ignore this parameter\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    train_raw = load_dataset(train_paths[0], train_paths[1])\n",
    "    dev_raw = load_dataset(dev_paths[0], dev_paths[1])\n",
    "    return train_raw, dev_raw\n",
    "\n",
    "def translation_preprocess(dataset, x_set, x, y):\n",
    "    if not (x in x_set):\n",
    "        y = y.replace('\\u202f', ' ')\n",
    "        x_set.add(x)\n",
    "        mydict = {}\n",
    "        for i in string.punctuation:\n",
    "            mydict[ord(i)] = None\n",
    "        x = x.translate(mydict)\n",
    "        y = y.translate(mydict) + ' <EOS>'\n",
    "        dataset.append((x, y))\n",
    "\n",
    "def load_dataset(x_filename:str, y_filename:str):\n",
    "    x_dataset = []\n",
    "    y_dataset = []\n",
    "    with open(x_filename) as f:\n",
    "        for line in f:\n",
    "            x_dataset.append(line.rstrip('\\n'))\n",
    "    with open(y_filename) as f:\n",
    "        for line in f:\n",
    "            y_dataset.append(line.rstrip('\\n') + ' <EOS>')\n",
    "\n",
    "    dataset = []\n",
    "    for i in range(len(x_dataset)):\n",
    "        dataset.append((x_dataset[i], y_dataset[i]))\n",
    "    return dataset\n",
    "\n",
    "def tokenize(x) -> List[str]:\n",
    "    \"\"\"\n",
    "    :param x: string to tokenize\n",
    "    :return: x tokenized with whitespace tokenization\n",
    "    \"\"\"\n",
    "    return x.split()\n",
    "\n",
    "\n",
    "def index(x_tok: List[str], indexer: Indexer) -> List[int]:\n",
    "    return [indexer.index_of(xi) if indexer.index_of(xi) >= 0 else indexer.index_of(UNK_SYMBOL) for xi in x_tok]\n",
    "\n",
    "\n",
    "def index_data(data, input_indexer: Indexer, output_indexer: Indexer, example_len_limit):\n",
    "    \"\"\"\n",
    "    Indexes the given data\n",
    "    :param data:\n",
    "    :param input_indexer:\n",
    "    :param output_indexer:\n",
    "    :param example_len_limit:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    data_indexed = []\n",
    "    for (x, y) in data:\n",
    "        x_tok = tokenize(x)\n",
    "        y_tok = tokenize(y)[0:example_len_limit]\n",
    "        data_indexed.append(Example(x, x_tok, index(x_tok, input_indexer), y, y_tok,\n",
    "                                          index(y_tok, output_indexer) + [output_indexer.index_of(EOS_SYMBOL)]))\n",
    "    return data_indexed\n",
    "\n",
    "\n",
    "def index_datasets(train_data, dev_data, example_len_limit, unk_threshold=0.0) -> (List[Example], List[Example], Indexer, Indexer):\n",
    "    \"\"\"\n",
    "    Indexes train and test datasets where all words occurring less than or equal to unk_threshold times are\n",
    "    replaced by UNK tokens.\n",
    "    :param train_data:\n",
    "    :param dev_data:\n",
    "    :param test_data:\n",
    "    :param example_len_limit:\n",
    "    :param unk_threshold: threshold below which words are replaced with unks. If 0.0, the model doesn't see any\n",
    "    UNKs at train time\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    input_word_counts = Counter()\n",
    "    # Count words and build the indexers\n",
    "    for (x, y) in train_data:\n",
    "        for word in tokenize(x):\n",
    "            input_word_counts[word] += 1.0\n",
    "    input_indexer = Indexer()\n",
    "    output_indexer = Indexer()\n",
    "    # Reserve 0 for the pad symbol for convenience\n",
    "    input_indexer.add_and_get_index(PAD_SYMBOL)\n",
    "    input_indexer.add_and_get_index(UNK_SYMBOL)\n",
    "    input_indexer.add_and_get_index(SOS_SYMBOL)\n",
    "    output_indexer.add_and_get_index(PAD_SYMBOL)\n",
    "    output_indexer.add_and_get_index(SOS_SYMBOL)\n",
    "    output_indexer.add_and_get_index(EOS_SYMBOL)\n",
    "    # Index all input words above the UNK threshold\n",
    "    for word in input_word_counts.keys():\n",
    "        if input_word_counts[word] > unk_threshold + 0.5:\n",
    "            input_indexer.add_and_get_index(word)\n",
    "    # Index all output tokens in train\n",
    "    for (x, y) in train_data:\n",
    "        for y_tok in tokenize(y):\n",
    "            output_indexer.add_and_get_index(y_tok)\n",
    "    # Index things\n",
    "    train_data_indexed = index_data(train_data, input_indexer, output_indexer, example_len_limit)\n",
    "    dev_data_indexed = index_data(dev_data, input_indexer, output_indexer, example_len_limit)\n",
    "    return train_data_indexed, dev_data_indexed, input_indexer, output_indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GqlQXofWIHHu"
   },
   "outputs": [],
   "source": [
    "train, dev = load_datasets((cwd+'data/trans_train.en', cwd+'data/trans_train.fr'),\n",
    "                           (cwd+'data/trans_test.en', cwd+'data/trans_test.fr'))\n",
    "train_data_indexed, dev_data_indexed, input_indexer, output_indexer = index_datasets(train, dev, 65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAms_AL3I4E0"
   },
   "source": [
    "### RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ZQ2CROsIRdu"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Embedding layer that has a lookup table of symbols that is [full_dict_size x input_dim]. Includes dropout.\n",
    "    Works for both non-batched and batched inputs\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, full_dict_size: int, embedding_dropout_rate: float):\n",
    "        \"\"\"\n",
    "        :param input_dim: dimensionality of the word vectors\n",
    "        :param full_dict_size: number of words in the vocabulary\n",
    "        :param embedding_dropout_rate: dropout rate to apply\n",
    "        \"\"\"\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.dropout = nn.Dropout(embedding_dropout_rate)\n",
    "        self.word_embedding = nn.Embedding(full_dict_size, input_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        :param input: either a non-batched input [sent len x voc size] or a batched input\n",
    "        [batch size x sent len x voc size]\n",
    "        :return: embedded form of the input words (last coordinate replaced by input_dim)\n",
    "        \"\"\"\n",
    "        embedded_words = self.word_embedding(inputs)\n",
    "        final_embeddings = self.dropout(embedded_words)\n",
    "        return final_embeddings\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, embedding_dropout, hidden_size: int, bidirect: bool):\n",
    "        \"\"\"\n",
    "        :param emb_dim: size of word embeddings output by embedding layer\n",
    "        :param hidden_size: hidden size for the LSTM\n",
    "        :param bidirect: True if bidirectional, false otherwise\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = EmbeddingLayer(emb_dim, vocab_size, embedding_dropout)\n",
    "        self.bidirect = bidirect\n",
    "        self.hidden_size = hidden_size\n",
    "        self.reduce_h_W = nn.Linear(hidden_size * 2, hidden_size, bias=True)\n",
    "        self.reduce_c_W = nn.Linear(hidden_size * 2, hidden_size, bias=True)\n",
    "        self.rnn = nn.LSTM(emb_dim, hidden_size, num_layers=1, batch_first=True,\n",
    "                               dropout=0., bidirectional=self.bidirect)\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        \"\"\"\n",
    "        Initializes weight matrices using Xavier initialization\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        nn.init.xavier_uniform_(self.rnn.weight_hh_l0, gain=1)\n",
    "        nn.init.xavier_uniform_(self.rnn.weight_ih_l0, gain=1)\n",
    "        if self.bidirect:\n",
    "            nn.init.xavier_uniform_(self.rnn.weight_hh_l0_reverse, gain=1)\n",
    "            nn.init.xavier_uniform_(self.rnn.weight_ih_l0_reverse, gain=1)\n",
    "        nn.init.constant_(self.rnn.bias_hh_l0, 0)\n",
    "        nn.init.constant_(self.rnn.bias_ih_l0, 0)\n",
    "        if self.bidirect:\n",
    "            nn.init.constant_(self.rnn.bias_hh_l0_reverse, 0)\n",
    "            nn.init.constant_(self.rnn.bias_ih_l0_reverse, 0)\n",
    "\n",
    "    def forward(self, x_tensor, input_lens):\n",
    "        embedded_words = self.embedding.forward(x_tensor)\n",
    "        packed_embedding = nn.utils.rnn.pack_padded_sequence(embedded_words, input_lens, batch_first=True, enforce_sorted=False)\n",
    "        output, hn = self.rnn(packed_embedding)\n",
    "        output, sent_lens = nn.utils.rnn.pad_packed_sequence(output)\n",
    "        max_length = torch.max(input_lens).item()\n",
    "\n",
    "        if self.bidirect:\n",
    "            h, c = hn[0], hn[1]\n",
    "            h_, c_ = torch.cat((h[0], h[1]), dim=1), torch.cat((c[0], c[1]), dim=1)\n",
    "            new_h = self.reduce_h_W(h_)\n",
    "            new_c = self.reduce_c_W(c_)\n",
    "            h_t = (new_h, new_c)\n",
    "        else:\n",
    "            h, c = hn[0][0], hn[1][0]\n",
    "            h_t = (h, c)\n",
    "        return (output, h_t)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, indexer, emb_dim, embedding_dropout, hidden_size: int, \n",
    "                 bidirect: bool, attention: bool):\n",
    "        \"\"\"\n",
    "        :param emb_dim: size of word embeddings output by embedding layer\n",
    "        :param hidden_size: hidden size for the LSTM\n",
    "        :param bidirect: True if bidirectional, false otherwise\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.indexer = indexer\n",
    "        self.embedding = EmbeddingLayer(emb_dim, len(indexer), embedding_dropout)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bidirect = bidirect\n",
    "        self.attention = attention\n",
    "        self.rnn = nn.LSTM(emb_dim, hidden_size, num_layers=1, batch_first=True,\n",
    "                           dropout=0., bidirectional=False)\n",
    "        self.reduce_h_W = nn.Linear(2 * hidden_size, hidden_size)\n",
    "        coef = 2 if attention else 1\n",
    "        self.h2o = nn.Linear(coef * hidden_size, len(indexer))\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.loss = nn.NLLLoss()\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        \"\"\"\n",
    "        Initializes weight matrices using Xavier initialization\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        nn.init.xavier_uniform_(self.rnn.weight_hh_l0, gain=1)\n",
    "        nn.init.xavier_uniform_(self.rnn.weight_ih_l0, gain=1)\n",
    "        nn.init.constant_(self.rnn.bias_hh_l0, 0)\n",
    "        nn.init.constant_(self.rnn.bias_ih_l0, 0)\n",
    "\n",
    "    def forward(self, x_tensor, hidden, encoder_outputs):\n",
    "        '''\n",
    "        hidden: (1 * batch * hid_size, 1 * batch * hid_size)\n",
    "        encoder_outputs: batch * 1 * vocab_size\n",
    "        '''\n",
    "        embedded_words = self.embedding.forward(x_tensor)\n",
    "        if self.bidirect:\n",
    "            encoder_outputs = self.reduce_h_W(encoder_outputs)\n",
    "        output, hn = self.rnn(embedded_words, hidden)\n",
    "        h, c = hn[0][0], hn[1][0] # h, c: batch * hid_size\n",
    "        if self.attention:\n",
    "            hid = h.unsqueeze(1) # batch * 1 * hid_size\n",
    "            enc_out = torch.swapaxes(encoder_outputs, 1, 2) # batch * hid_size * seq_len\n",
    "            attn_dot_product = torch.matmul(hid, enc_out).squeeze(1)\n",
    "            # attn_dot_product: batch * seq_len\n",
    "            attn_weight = F.softmax(attn_dot_product, dim=1)\n",
    "            # attn_weight: batch * seq_len\n",
    "            context = torch.matmul(attn_weight.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "            # context: batch * hid_size\n",
    "            out = torch.cat([h, context], dim=1)\n",
    "            # out: batch * (2 * hid_size)\n",
    "            output = self.softmax(self.h2o(out)).unsqueeze(1)\n",
    "        else:\n",
    "            output = self.softmax(self.h2o(output))\n",
    "\n",
    "        h_t = (h, c)\n",
    "        return (output, h_t)\n",
    "\n",
    "class Seq2Seq_Model(nn.Module):\n",
    "    def __init__(self, input_indexer, output_indexer, emb_dim, hidden_size, \n",
    "                 attention=False, embedding_dropout=0.2, bidirect=True, \n",
    "                 reverse_input=False, out_max_length=65, k=0, lambda2=0):\n",
    "        super(Seq2Seq_Model, self).__init__()\n",
    "        self.input_indexer = input_indexer\n",
    "        self.out_max_length = out_max_length\n",
    "        self.encoder = Encoder(len(input_indexer), emb_dim, embedding_dropout, \n",
    "                               hidden_size, bidirect)\n",
    "        self.decoder = Decoder(output_indexer, emb_dim, embedding_dropout, \n",
    "                               hidden_size, bidirect, attention)\n",
    "        self.loss = nn.NLLLoss()\n",
    "        self.lambda2 = lambda2\n",
    "        self.k = k\n",
    "\n",
    "    def sent_lens_to_mask(self, lens, max_length):\n",
    "        return torch.from_numpy(np.asarray([[1 if j < lens.data[i].item() else 0 for j in range(0, max_length)] for i in range(0, lens.shape[0])]))\n",
    "\n",
    "    def mask(self, vector, context_mask, seq_idx):\n",
    "        for i in range(len(context_mask)):\n",
    "            vector[i] *= context_mask[i, seq_idx]\n",
    "        return vector\n",
    "\n",
    "    def encode_input(self, x_tensor, inp_lens_tensor):\n",
    "        (enc_output_each_word, enc_final_states) = self.encoder.forward(x_tensor, inp_lens_tensor)\n",
    "        enc_final_states_reshaped = (enc_final_states[0].unsqueeze(0), enc_final_states[1].unsqueeze(0))\n",
    "        return (enc_output_each_word, enc_final_states_reshaped)\n",
    "\n",
    "    def dirichlet_dist(self, probs):\n",
    "        # apply dirichlet distribution to the probabilities\n",
    "        if self.k == 0:\n",
    "            return probs\n",
    "        probs = probs.detach().numpy()\n",
    "        for i in range(len(probs)):\n",
    "            for j in range(len(probs[0])):\n",
    "                alpha = np.exp(probs[i, j]) * self.k\n",
    "                rv = dirichlet.rvs(alpha, size=1, random_state=None)[0]\n",
    "                probs[i, j] = rv\n",
    "        return torch.Tensor(probs)\n",
    "\n",
    "    def forward(self, x_tensor, inp_lens_tensor, y_tensor, out_lens_tensor):\n",
    "        \"\"\"\n",
    "        :param x_tensor/y_tensor: either a non-batched input/output [sent len x voc size] or a batched input/output\n",
    "        [batch size x sent len x voc size]\n",
    "        :param inp_lens_tensor/out_lens_tensor: either a vecor of input/output length [batch size] or a single integer.\n",
    "        lengths aren't needed if you don't batchify the training.\n",
    "        :return: loss of the batch\n",
    "        \"\"\"\n",
    "        max_length = torch.max(out_lens_tensor).item()\n",
    "        context_mask = self.sent_lens_to_mask(out_lens_tensor, max_length)\n",
    "        # context_mask: batch * max_seq_len\n",
    "        loss = 0\n",
    "        enc_output_each_word, state = self.encode_input(x_tensor, inp_lens_tensor)\n",
    "        # max_seq_len * batch * hid_size, batch * max_seq_len, batch * hid_size\n",
    "        enc_output_each_word = torch.swapaxes(enc_output_each_word, 0, 1)\n",
    "\n",
    "        word_idx = self.decoder.indexer.index_of(SOS_SYMBOL)\n",
    "        word = torch.ones(len(x_tensor), 1, dtype=torch.int) * word_idx\n",
    "        for i in range(max_length):\n",
    "            output, state = self.decoder(word, state, enc_output_each_word)\n",
    "            # output: batch * 1 * vocab_size\n",
    "            output = self.mask(output.clone(), context_mask, i)\n",
    "            labels = y_tensor[:, i]\n",
    "            loss += self.loss(output.squeeze(1), labels)\n",
    "            word = labels.unsqueeze(1)\n",
    "            state = (state[0].unsqueeze(0), state[1].unsqueeze(0))\n",
    "        # L2 Regularization\n",
    "        l2_regularization = 0\n",
    "        if self.lambda2 > 0:\n",
    "            all_linear_params = torch.cat([x.view(-1) for x in self.decoder.rnn.parameters()])\n",
    "            l2_regularization = self.lambda2 * torch.norm(all_linear_params, 2)\n",
    "        return loss + l2_regularization\n",
    "\n",
    "    def decode(self, test_data: List[Example]) -> List[List[Derivation]]:\n",
    "        derivations = []\n",
    "        max_length = np.max(np.array([len(ex.x_indexed) for ex in test_data]))\n",
    "        input_data = make_padded_input_tensor(test_data, self.input_indexer, max_length)\n",
    "        for i in range(len(test_data)):\n",
    "            ex = test_data[i]\n",
    "            x_tensor = torch.from_numpy(input_data[i]).unsqueeze(0)\n",
    "            inp_lens_tensor = torch.from_numpy(np.array(len(test_data[i].x_indexed))).unsqueeze(0)\n",
    "            # encode the data\n",
    "            enc_outputs, state = self.encode_input(x_tensor, inp_lens_tensor)\n",
    "            # enc_outputs: 1 * batch * hid_size, state: 1 * hid_size\n",
    "            enc_outputs = torch.swapaxes(enc_outputs, 0, 1)\n",
    "\n",
    "            word = torch.ones(len(state[0]), 1, dtype=torch.int) * self.decoder.indexer.index_of(SOS_SYMBOL)\n",
    "            word_idx = self.decoder.indexer.index_of(SOS_SYMBOL)\n",
    "            length = 0\n",
    "            y_toks = []\n",
    "            probability_list = []\n",
    "            while length < self.out_max_length:\n",
    "                output, state = self.decoder(word, state, enc_outputs)\n",
    "                output = self.dirichlet_dist(output)\n",
    "                # output: 1 * 1 * vocab_size\n",
    "                word_idx = torch.argmax(output[0][0])\n",
    "                probability_list.append(output[0][0].detach().numpy())\n",
    "\n",
    "                # stop when hit the EOS token\n",
    "                if word_idx == self.decoder.indexer.index_of(EOS_SYMBOL):\n",
    "                    break\n",
    "                y_toks.append(self.decoder.indexer.get_object(word_idx.item()))\n",
    "                word = torch.tensor([word_idx]).unsqueeze(1)\n",
    "                state = (state[0].unsqueeze(0), state[1].unsqueeze(0))\n",
    "                length += 1\n",
    "            derivations.append(Derivation(ex, np.array(probability_list), y_toks))\n",
    "        return derivations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYg0eRuUO1cM"
   },
   "source": [
    "### Traning RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQfqECYxO4e7"
   },
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 200\n",
    "EMBEDDING_DIM = 150\n",
    "LR = 0.001\n",
    "BATCH_SIZE = 25\n",
    "DROPOUT = 0\n",
    "MAX_LEN = 65\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def train_model(train_data: List[Example], input_indexer, output_indexer,\n",
    "                name='ml_model', epochs=20, k=0, lambda2=0, seed=0):\n",
    "    EPOCH = epochs\n",
    "#     torch.manual_seed(seed)\n",
    "    if exists(cwd+'models/'+name):\n",
    "        return torch.load(cwd+'models/'+name)\n",
    "\n",
    "    model = Seq2Seq_Model(input_indexer, output_indexer, EMBEDDING_DIM, HIDDEN_SIZE, \n",
    "                          attention=True, embedding_dropout=DROPOUT, k=k, lambda2=lambda2)\n",
    "    # print(count_parameters(model))\n",
    "    # return\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    for i in range(EPOCH):\n",
    "        random.shuffle(train_data)\n",
    "        total_loss = 0\n",
    "        for j in range(0, len(train_data), BATCH_SIZE):\n",
    "            model.zero_grad()\n",
    "            batch_exs = train_data[j: j+BATCH_SIZE]\n",
    "\n",
    "            x_inp_len, y_inp_len = [],[]\n",
    "            for ex in batch_exs:\n",
    "                x_inp_len.append(len(ex.x_tok))\n",
    "                y_inp_len.append(len(ex.y_tok)+1) # include EOS\n",
    "            x_tensor = make_padded_input_tensor(batch_exs, input_indexer, max(x_inp_len), reverse_input=False)\n",
    "            y_tensor = make_padded_output_tensor(batch_exs, output_indexer, max(y_inp_len))\n",
    "            x_inp_len, y_inp_len = torch.tensor(x_inp_len), torch.tensor(y_inp_len)\n",
    "\n",
    "            loss = model.forward(torch.tensor(x_tensor), x_inp_len, torch.tensor(y_tensor), y_inp_len)\n",
    "            total_loss += loss\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print('Epoch ', i+1, total_loss)\n",
    "    torch.save(model, cwd+'models/'+name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wdZPYPEmQDkd",
    "outputId": "57b9a2d6-4bf4-41f4-9bae-412eaaeb529d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 tensor(2058.9829, grad_fn=<AddBackward0>)\n",
      "Epoch  2 tensor(1498.0057, grad_fn=<AddBackward0>)\n",
      "Epoch  3 tensor(1357.8638, grad_fn=<AddBackward0>)\n",
      "Epoch  4 tensor(1242.9484, grad_fn=<AddBackward0>)\n",
      "Epoch  5 tensor(1134.5123, grad_fn=<AddBackward0>)\n",
      "Epoch  6 tensor(1037.9434, grad_fn=<AddBackward0>)\n",
      "Epoch  7 tensor(956.4129, grad_fn=<AddBackward0>)\n",
      "Epoch  8 tensor(873.2903, grad_fn=<AddBackward0>)\n",
      "Epoch  9 tensor(798.5353, grad_fn=<AddBackward0>)\n",
      "Epoch  10 tensor(723.4243, grad_fn=<AddBackward0>)\n",
      "Epoch  11 tensor(647.9341, grad_fn=<AddBackward0>)\n",
      "Epoch  12 tensor(578.6802, grad_fn=<AddBackward0>)\n",
      "Epoch  13 tensor(507.8611, grad_fn=<AddBackward0>)\n",
      "Epoch  14 tensor(453.5694, grad_fn=<AddBackward0>)\n",
      "Epoch  15 tensor(395.0979, grad_fn=<AddBackward0>)\n",
      "Epoch  16 tensor(337.7388, grad_fn=<AddBackward0>)\n",
      "Epoch  17 tensor(302.7875, grad_fn=<AddBackward0>)\n",
      "Epoch  18 tensor(263.0762, grad_fn=<AddBackward0>)\n",
      "Epoch  19 tensor(235.0412, grad_fn=<AddBackward0>)\n",
      "Epoch  20 tensor(202.1002, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "shadow_model = train_model(train_data_indexed[:5000], input_indexer, output_indexer, name='ml_model', epochs=20)\n",
    "# target_model = train_model(train_data_indexed[:5000], input_indexer, output_indexer, 'ml_target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pli-o5vzOU5-"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H3BeiPtvNh7w"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "def bleu_score(pred, label):\n",
    "    '''\n",
    "    pred, label: list of strings (tokens)\n",
    "    '''\n",
    "    reference = [pred]\n",
    "    candidate = label[:-1]\n",
    "    smoothie = SmoothingFunction().method3\n",
    "    score = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0), smoothing_function=smoothie)\n",
    "    return score\n",
    "\n",
    "def compute_entropy(test_data: List[Example], decoder, single=False):\n",
    "    entropies = []\n",
    "    pred_derivations = decoder.decode(test_data)\n",
    "    for derivs in pred_derivations:\n",
    "        entropy = 0\n",
    "        arr = np.exp(derivs.p)\n",
    "        for i in arr:\n",
    "            entropy += scipy.stats.entropy(i, base=len(i))\n",
    "        entropies.append(entropy/len(arr))\n",
    "    if single:\n",
    "        return entropies\n",
    "    return sum(entropies)/len(entropies)\n",
    "\n",
    "def CrossEntropy(test_data: List[Example], decoder):\n",
    "    entropies = []\n",
    "    pred_derivations = decoder.decode(test_data)\n",
    "    for j in range(len(pred_derivations)):\n",
    "        derivs = pred_derivations[j]\n",
    "        label = test_data[j].y_indexed\n",
    "        entropy = 0\n",
    "        arr = np.exp(derivs.p)\n",
    "        leng = min(len(arr), len(label))\n",
    "        for i in range(leng):\n",
    "            entropy -= np.log(arr[i][label[i]])\n",
    "        entropies.append(entropy/leng)\n",
    "    return entropies\n",
    "\n",
    "def evaluate_ml(test_data: List[Example], decoder):\n",
    "    avg_bleu = 0\n",
    "    pred_derivations = decoder.decode(test_data)\n",
    "    for derivs in pred_derivations:\n",
    "        label = derivs.example.y_tok\n",
    "        pred = derivs.y_toks\n",
    "        avg_bleu += bleu_score(pred, label)\n",
    "    print('BLEU Score: ', avg_bleu/len(test_data))\n",
    "    return avg_bleu/len(test_data)\n",
    "\n",
    "def neg(arr):\n",
    "    for i in range(len(arr)):\n",
    "        arr[i] = False if arr[i] else True\n",
    "\n",
    "evaluate_ml(train_data_indexed[:500], shadow_model)\n",
    "evaluate_ml(dev_data_indexed, shadow_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "glmpKDppiR6p"
   },
   "outputs": [],
   "source": [
    "en_train = compute_entropy(train_data_indexed[450:500], shadow_model, True)\n",
    "en_test = compute_entropy(dev_data_indexed[:50], shadow_model, True)\n",
    "en = np.array(en_train + en_test)\n",
    "\n",
    "ce1 = CrossEntropy(train_data_indexed[:50], shadow_model)\n",
    "ce2 = CrossEntropy(dev_data_indexed[:50], shadow_model)\n",
    "ce = np.array(ce1 + ce2)\n",
    "\n",
    "lb = [en[i]>0.2 or ce[i] > 4 for i in range(len(en))]\n",
    "# lb = [preds[i][i] > 0.5 for i in range(len(en))]\n",
    "\n",
    "plt.scatter(en[lb], ce[lb], color='red', marker='.', label='out')\n",
    "neg(lb)\n",
    "plt.scatter(en[lb], ce[lb], color='blue', marker='.', label='in')\n",
    "plt.xlabel('RNN Entropy')\n",
    "plt.ylabel('Cross Entropy Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Privacy Budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j36UhgSwOZp9",
    "outputId": "2fb2d070-7f81-4192-c744-afe1a2b3fa0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score:  0.4636494832986608\n",
      "epsilon=  6.54  \n",
      "BLEU Score:  0.4613629352094867\n",
      "Utility loss:  0.004931630836524037\n",
      "epsilon=  2.18  \n",
      "BLEU Score:  0.4569261786753715\n",
      "Utility loss:  0.014500834931284623\n",
      "epsilon=  1.3079999999999998  \n",
      "BLEU Score:  0.4391119211494958\n",
      "Utility loss:  0.05292265608621216\n",
      "epsilon=  0.9342857142857142  \n",
      "BLEU Score:  0.3943725191151318\n",
      "Utility loss:  0.14941667505084666\n",
      "epsilon=  0.7266666666666667  \n",
      "BLEU Score:  0.35769105909325183\n",
      "Utility loss:  0.228531310876401\n",
      "epsilon=  0.5945454545454545  \n",
      "BLEU Score:  0.23532632756226035\n",
      "Utility loss:  0.492447773503342\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "shadow_model = torch.load(cwd+'models/ml_model')\n",
    "# target_model = torch.load(cwd+'models/ml_target')\n",
    "\n",
    "# evaluate_ml(train_data_indexed[:500], shadow_model)\n",
    "base = evaluate_ml(dev_data_indexed, shadow_model)\n",
    "\n",
    "for i in [0.01, 0.03, 0.05, 0.07, 0.09, 0.11]:\n",
    "    print('epsilon= ', 4.36*0.015/i, ' ')\n",
    "    shadow_model = torch.load(cwd+'models/ml_model')\n",
    "    model_copy = shadow_model.decoder\n",
    "    for param in model_copy.state_dict():\n",
    "        size = model_copy.state_dict()[param].shape\n",
    "        model_copy.state_dict()[param] += torch.Tensor(np.random.normal(0, i, size))\n",
    "    shadow_model.decoder = model_copy\n",
    "    acc = evaluate_ml(dev_data_indexed, shadow_model)\n",
    "    print('Utility loss: ', 1 - acc/base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ilYejFwQDYR"
   },
   "source": [
    "### Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "089z9jgp4EQk"
   },
   "outputs": [],
   "source": [
    "shadow_model = train_model(train_data_indexed[:500], input_indexer, output_indexer, name='model_fed_1')\n",
    "model2 = train_model(train_data_indexed[1:501], input_indexer, output_indexer, name='model_fed_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qXDRXTXl-dXE",
    "outputId": "26506ae4-7387-4222-f8cb-9ccac3b642d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011670646\n"
     ]
    }
   ],
   "source": [
    "def sensitivity_sampler(train, sample_size, contribution=1):\n",
    "    sens = []\n",
    "    model_1 = train_model(train, input_indexer, output_indexer, \n",
    "                              'ml_sen', seed=0)\n",
    "    for i in range(sample_size):\n",
    "        D = int(np.random.uniform(0, len(train)-contribution))\n",
    "        model_2 = train_model(train[:D]+train[D+contribution:], \n",
    "                              input_indexer, output_indexer, \n",
    "                              'ml_sen_'+str(D), seed=0)\n",
    "        sens.append(sensitivity(model_1, model_2))\n",
    "        return max(sens)\n",
    "\n",
    "def sensitivity(model_1, model_2, norm=2):\n",
    "    sen=0\n",
    "    data_vec_1 = [i for i in model_1.state_dict()]\n",
    "    data_vec_2 = [i for i in model_2.state_dict()]\n",
    "    for i in range(len(data_vec_1)):\n",
    "        n1, n2 = data_vec_1[i], data_vec_2[i]\n",
    "        vec_1, vec_2 = model_1.state_dict()[n1], model_2.state_dict()[n2]\n",
    "        diff = (vec_1 - vec_2).cpu().detach().numpy().flatten()\n",
    "        sen += np.linalg.norm(diff, norm)\n",
    "    return sen\n",
    "\n",
    "sensitivity_sampler(train_data_indexed[1000:], 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcc6balhwlws"
   },
   "source": [
    "### Membership Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-7xVSe9vTSu"
   },
   "outputs": [],
   "source": [
    "def get_label_vector(labels, vocab_size):\n",
    "    label_vectors = np.zeros((len(labels), vocab_size))\n",
    "    for i in range(len(labels)):\n",
    "        label_vectors[i, labels[i]] = 1\n",
    "    return label_vectors\n",
    "\n",
    "def get_att_data(in_data, out_data):\n",
    "    in_label = [1.0]*len(in_data)\n",
    "    out_label = [0.0]*len(out_data)\n",
    "    labels = in_label + out_label\n",
    "    in_data = [d for d in in_data]\n",
    "    out_data = [d for d in out_data]\n",
    "    data = in_data + out_data\n",
    "\n",
    "    c = list(zip(data, labels))\n",
    "    random.shuffle(c)\n",
    "    data, labels = zip(*c)\n",
    "    return data, labels\n",
    "    # return np.array(data), np.array(labels)\n",
    "\n",
    "def label_vectors(train_data_indexed, dev_data_indexed, model):\n",
    "    labels = [ex.y_indexed for ex in train_data_indexed] + [ex.y_indexed for ex in dev_data_indexed]\n",
    "    label_vec = []\n",
    "    for lab in labels:\n",
    "        label_vec.append(get_label_vector(lab, len(model.decoder.indexer)))\n",
    "\n",
    "    # label_vec = np.array(label_vec)\n",
    "    # label_vec = np.swapaxes(label_vec, 1, 2)\n",
    "    return label_vec\n",
    "\n",
    "def output_vectors(train_data_indexed, dev_data_indexed, model):\n",
    "    data = train_data_indexed + dev_data_indexed\n",
    "    derivations = model.decode(data)\n",
    "\n",
    "    # data_vec = np.zeros((len(data), 6, len(derivations[0].p[0])))\n",
    "    data_vec = []\n",
    "    i = 0\n",
    "    for d in derivations:\n",
    "        # data_vec[i, -len(d.p[-6:]):] = d.p[-6:]\n",
    "        data_vec.append(d.p)\n",
    "        i += 1\n",
    "    # data_vec = np.swapaxes(data_vec, 1, 2)\n",
    "    # data_vec[np.isnan(data_vec)] = 0\n",
    "    return data_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "buzlW1qdxqNV",
    "outputId": "c762fda8-299b-43a4-f8cd-425229582e7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 3091)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_vec = label_vectors(train_data_indexed[:5000], dev_data_indexed, shadow_model)\n",
    "label_vec[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4hWbTGjczVXV",
    "outputId": "1edfb7a5-df7a-466e-90bf-8e2bdef68767"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 3091)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_vec = output_vectors(train_data_indexed[:5000], dev_data_indexed, shadow_model)\n",
    "data_vec[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AODX_T59zv4y"
   },
   "outputs": [],
   "source": [
    "all_data = [np.concatenate([label_vec[i], data_vec[i]], axis=0) for i in range(len(label_vec))]\n",
    "data, labels = get_att_data(all_data[:5000], all_data[5000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sq0N9zkg07Ba"
   },
   "outputs": [],
   "source": [
    "class RNN_Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1):\n",
    "        super(RNN_Classifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers)\n",
    "        self.hid2out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        nn.init.xavier_uniform_(self.hid2out.weight)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        state = (torch.from_numpy(np.zeros((self.num_layers, len(inputs), self.hidden_dim))).float(),\n",
    "                 torch.from_numpy(np.zeros((self.num_layers, len(inputs), self.hidden_dim))).float())\n",
    "        inputs = nn.utils.rnn.pad_sequence(inputs)\n",
    "        hidden, (hidden_state, cell_state) = self.lstm(inputs, state)\n",
    "        hidden = hidden[-1]\n",
    "        output = self.softmax(self.hid2out(hidden))\n",
    "        return output\n",
    "\n",
    "def evaluate_attack(data, label, model):\n",
    "    pred = []\n",
    "    batch_size = 100\n",
    "    for j in range(0, len(data), batch_size):\n",
    "        batch_input = data[j:j+batch_size]\n",
    "        batch_input = [torch.FloatTensor(ex) for ex in batch_input]\n",
    "        results = model(batch_input).tolist()\n",
    "        pred += [np.argmax(result) for result in results]\n",
    "    return metrics.accuracy_score(label, pred)\n",
    "\n",
    "def train_attack(data, label, valid_data, valid_label, batch_size=10, epochs=10, \n",
    "                 name='att_model', verbose=True):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model = RNN_Classifier(data[0].shape[1], 100, 2)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    cur_acc = 0\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        total_loss = 0\n",
    "        for j in range(0, len(data), batch_size):\n",
    "            model.zero_grad()\n",
    "            batch_words = data[j: j+batch_size]\n",
    "            batch_labels = label[j: j+batch_size]\n",
    "            batch_words = [torch.FloatTensor(ex) for ex in batch_words]\n",
    "            output = model.forward(batch_words)\n",
    "            labels = torch.tensor(batch_labels, dtype=torch.long)\n",
    "            loss = criterion(output, labels)\n",
    "            total_loss += loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        acc = evaluate_attack(valid_data, valid_label, model)\n",
    "        if acc > cur_acc:\n",
    "            torch.save(model, cwd+'models/' + name)\n",
    "            cur_acc = acc\n",
    "        if verbose:\n",
    "            print(i+1, total_loss, acc)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EkGCUwexMjNM"
   },
   "outputs": [],
   "source": [
    "label_vec = label_vectors(train_data_indexed[5000:10000], dev_data_indexed, target_model)\n",
    "data_vec = output_vectors(train_data_indexed[5000:10000], dev_data_indexed, target_model)\n",
    "all_data = [np.concatenate([label_vec[i], data_vec[i]], axis=0) for i in range(len(label_vec))]\n",
    "v_data, v_labels = get_att_data(all_data[:5000], all_data[5000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "007aG6ZT7Hi1",
    "outputId": "2410c462-6db4-4f7f-9021-5d47c5a2d4b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tensor(71.2521, grad_fn=<AddBackward0>) 0.492\n",
      "2 tensor(70.1890, grad_fn=<AddBackward0>) 0.503\n",
      "3 tensor(67.4297, grad_fn=<AddBackward0>) 0.706\n",
      "4 tensor(54.0872, grad_fn=<AddBackward0>) 0.72\n",
      "5 tensor(48.7388, grad_fn=<AddBackward0>) 0.721\n",
      "6 tensor(43.2599, grad_fn=<AddBackward0>) 0.784\n",
      "7 tensor(38.8280, grad_fn=<AddBackward0>) 0.784\n",
      "8 tensor(33.1551, grad_fn=<AddBackward0>) 0.805\n",
      "9 tensor(28.8777, grad_fn=<AddBackward0>) 0.792\n",
      "10 tensor(26.1293, grad_fn=<AddBackward0>) 0.794\n",
      "11 tensor(26.3708, grad_fn=<AddBackward0>) 0.812\n",
      "12 tensor(23.6370, grad_fn=<AddBackward0>) 0.809\n",
      "13 tensor(19.5084, grad_fn=<AddBackward0>) 0.805\n",
      "14 tensor(18.9966, grad_fn=<AddBackward0>) 0.787\n",
      "15 tensor(19.3465, grad_fn=<AddBackward0>) 0.798\n",
      "16 tensor(16.8658, grad_fn=<AddBackward0>) 0.796\n",
      "17 tensor(17.1696, grad_fn=<AddBackward0>) 0.81\n",
      "18 tensor(14.8928, grad_fn=<AddBackward0>) 0.821\n",
      "19 tensor(15.1332, grad_fn=<AddBackward0>) 0.825\n",
      "20 tensor(14.4882, grad_fn=<AddBackward0>) 0.828\n"
     ]
    }
   ],
   "source": [
    "attack_model = train_attack(data, labels, v_data, v_labels, batch_size=10, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfLfCTQ8G6h7"
   },
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYK5OEvrHEMK"
   },
   "source": [
    "### Performance vs. Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mc0vkNj6ObvA",
    "outputId": "8e980e06-01e3-4dc0-f04b-f8476390fb92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 tensor(37167.3828, grad_fn=<AddBackward0>)\n",
      "Epoch  2 tensor(26521.4648, grad_fn=<AddBackward0>)\n",
      "Epoch  3 tensor(20305.6094, grad_fn=<AddBackward0>)\n",
      "Epoch  4 tensor(14937.4414, grad_fn=<AddBackward0>)\n",
      "Epoch  5 tensor(9953.8115, grad_fn=<AddBackward0>)\n",
      "Epoch  1 tensor(37107.1016, grad_fn=<AddBackward0>)\n",
      "Epoch  2 tensor(26983.3066, grad_fn=<AddBackward0>)\n",
      "Epoch  3 tensor(20794.7520, grad_fn=<AddBackward0>)\n",
      "Epoch  4 tensor(15074.1670, grad_fn=<AddBackward0>)\n",
      "Epoch  5 tensor(10137.3984, grad_fn=<AddBackward0>)\n",
      "Epoch  6 tensor(6787.7090, grad_fn=<AddBackward0>)\n",
      "Epoch  7 tensor(4659.5103, grad_fn=<AddBackward0>)\n",
      "Epoch  8 tensor(3267.7966, grad_fn=<AddBackward0>)\n",
      "Epoch  9 tensor(2491.4827, grad_fn=<AddBackward0>)\n",
      "Epoch  10 tensor(2167.6162, grad_fn=<AddBackward0>)\n",
      "Epoch  1 tensor(36829.6719, grad_fn=<AddBackward0>)\n",
      "Epoch  2 tensor(26350.1426, grad_fn=<AddBackward0>)\n",
      "Epoch  3 tensor(20207.7832, grad_fn=<AddBackward0>)\n",
      "Epoch  4 tensor(14642.1074, grad_fn=<AddBackward0>)\n",
      "Epoch  5 tensor(9952.8232, grad_fn=<AddBackward0>)\n",
      "Epoch  6 tensor(6104.3223, grad_fn=<AddBackward0>)\n",
      "Epoch  7 tensor(4084.7729, grad_fn=<AddBackward0>)\n",
      "Epoch  8 tensor(2684.6396, grad_fn=<AddBackward0>)\n",
      "Epoch  9 tensor(2500.3225, grad_fn=<AddBackward0>)\n",
      "Epoch  10 tensor(1791.2805, grad_fn=<AddBackward0>)\n",
      "Epoch  11 tensor(1285.7817, grad_fn=<AddBackward0>)\n",
      "Epoch  12 tensor(1199.6725, grad_fn=<AddBackward0>)\n",
      "Epoch  13 tensor(1257.7837, grad_fn=<AddBackward0>)\n",
      "Epoch  14 tensor(978.9576, grad_fn=<AddBackward0>)\n",
      "Epoch  15 tensor(838.5215, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for i in [5, 10, 15]:\n",
    "    models.append(train_model(train_data_indexed[:5000], input_indexer, \n",
    "                              output_indexer, 'ml_'+str(i), i))\n",
    "models.append(shadow_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ORxV0Q9Zagz-",
    "outputId": "f5d74e2c-53ad-4214-e4ed-a07cf75a9a16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score:  0.553578344598289\n",
      "BLEU Score:  0.38220604626575294\n",
      "\n",
      "BLEU Score:  0.8695712942568329\n",
      "BLEU Score:  0.45121684809735524\n",
      "\n",
      "BLEU Score:  0.9409331976694416\n",
      "BLEU Score:  0.4519659698787222\n",
      "\n",
      "BLEU Score:  0.9461026907862735\n",
      "BLEU Score:  0.4578782334475625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for m in models:\n",
    "    evaluate_ml(train_data_indexed[:5000], m)\n",
    "    evaluate_ml(dev_data_indexed, m)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80EHgYWCUVb-"
   },
   "source": [
    "#### Attack Accuracy vs. Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ttbPzDSmUZxD",
    "outputId": "941a3ae5-520d-4b04-a090-2bac9dd2c82c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score:  0.4129391288582746\n",
      "BLEU Score:  0.43447608346284095\n",
      "BLEU Score:  0.4477544261457854\n",
      "BLEU Score:  0.45493126141991097\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for i in [500, 1000, 1500, 2000]:\n",
    "    models.append(train_model(train_data_indexed[:i], input_indexer, \n",
    "                              output_indexer, 'ml_ds_'+str(i), epochs=10))\n",
    "    \n",
    "for m in models:\n",
    "    evaluate_ml(dev_data_indexed, m)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "REhi1WsAV6x0",
    "outputId": "c6bd0e79-477e-43a2-e38f-23e4244a3cd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack Accuracy:  0.8582\n",
      "Attack Accuracy:  0.8364\n",
      "Attack Accuracy:  0.8197\n",
      "Attack Accuracy:  0.8103\n"
     ]
    }
   ],
   "source": [
    "for m in models:\n",
    "    label_vec = label_vectors(train_data_indexed[:5000], dev_data_indexed, m)\n",
    "    data_vec = output_vectors(train_data_indexed[:5000], dev_data_indexed, m)\n",
    "    all_data = [np.concatenate([label_vec[i], data_vec[i]], axis=0) for i in range(len(label_vec))]\n",
    "    v_data, v_labels = get_att_data(all_data[:5000], all_data[5000:])\n",
    "    acc = evaluate_attack(v_data, v_labels, attack_model)\n",
    "    print('Attack Accuracy: ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5QDsrXvHvTPl"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "pcc6balhwlws",
    "_ZfstzjfTCYR",
    "3NhPTKTjTEHc",
    "YCP2znCSD04S",
    "BfLfCTQ8G6h7"
   ],
   "name": "Mem_att_mt.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
