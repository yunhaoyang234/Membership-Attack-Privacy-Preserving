{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7BGYRK2eF5G0",
    "outputId": "6817873d-addd-4412-da43-0fc883c6d618"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n",
      "1.11.0+cu113\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "print(torch.__version__)\n",
    "\n",
    "# keras\n",
    "import keras\n",
    "from keras.layers import Input, Dense, Conv2D, Dropout, Activation, Flatten, Reshape, Softmax, MaxPooling2D\n",
    "from keras.models import Model, Sequential, clone_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# scikit-learn\n",
    "import sklearn\n",
    "from sklearn import cluster, decomposition, manifold, metrics\n",
    "import pandas as pd\n",
    "\n",
    "import scipy\n",
    "from scipy.stats import entropy\n",
    "from scipy.stats import dirichlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mVDw3R-e4Ssk",
    "outputId": "6d1025f4-6cd0-45ac-ce44-bf40d81a0f3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting einops\n",
      "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
      "Installing collected packages: einops\n",
      "Successfully installed einops-0.4.1\n",
      "Collecting kornia\n",
      "  Downloading kornia-0.6.4-py2.py3-none-any.whl (493 kB)\n",
      "\u001b[K     |████████████████████████████████| 493 kB 4.4 MB/s \n",
      "\u001b[?25hRequirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from kornia) (1.11.0+cu113)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from kornia) (21.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.8.1->kornia) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->kornia) (3.0.8)\n",
      "Installing collected packages: kornia\n",
      "Successfully installed kornia-0.6.4\n"
     ]
    }
   ],
   "source": [
    "!pip install einops\n",
    "!pip install kornia\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from contextlib import contextmanager\n",
    "import torchvision\n",
    "import torch\n",
    "from torchvision.datasets import MNIST,CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "from six import add_metaclass\n",
    "from torch.nn import init\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import datasets, transforms\n",
    "from contextlib import contextmanager\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, RandomCrop\n",
    "from six import add_metaclass\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "\n",
    "from kornia.augmentation import RandomCrop, Normalize\n",
    "from argparse import ArgumentParser\n",
    "import errno\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ul4v2NWy5QZX"
   },
   "source": [
    "### ReNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fH5vkKgq5P1b",
    "outputId": "0d4890db-3828-49b7-ddbf-5195be582235"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init-weight_ih_l0\n",
      "Init-weight_hh_l0\n",
      "Init-weight_ih_l0_reverse\n",
      "Init-weight_hh_l0_reverse\n",
      "Init-weight_ih_l0\n",
      "Init-weight_hh_l0\n",
      "Init-weight_ih_l0_reverse\n",
      "Init-weight_hh_l0_reverse\n",
      "Init-weight_ih_l0\n",
      "Init-weight_hh_l0\n",
      "Init-weight_ih_l0_reverse\n",
      "Init-weight_hh_l0_reverse\n",
      "Init-weight_ih_l0\n",
      "Init-weight_hh_l0\n",
      "Init-weight_ih_l0_reverse\n",
      "Init-weight_hh_l0_reverse\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42569590"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def weights_init(m):\n",
    "    # Code taken from https://discuss.pytorch.org/t/initializing-rnn-gru-and-lstm-correctly/23605/8\n",
    "    parameters = m.state_dict()\n",
    "    for each_key in parameters.keys():\n",
    "        print(f'Init-{each_key}')\n",
    "        if 'weight_ih' in each_key:\n",
    "            nn.init.orthogonal_(parameters[each_key])\n",
    "        elif 'weight_hh' in each_key:\n",
    "            nn.init.orthogonal_(parameters[each_key])\n",
    "        elif 'bias' in each_key:\n",
    "            nn.init.constant_(parameters[each_key], val=0)\n",
    "\n",
    "class ReNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, kernel_size=(2, 2), rnn='GRU', depth=(1,1)):\n",
    "        super(ReNet, self).__init__()\n",
    "        if rnn == 'GRU':\n",
    "            rnn = nn.GRU\n",
    "        elif rnn == 'LSTM':\n",
    "            rnn = nn.LSTM\n",
    "        \n",
    "        self.lstm_h = rnn(input_size, hidden_size, bias=False, num_layers=depth[0], bidirectional=True)\n",
    "        self.lstm_v = rnn(hidden_size * 2, hidden_size, bias=False, num_layers=depth[1], bidirectional=True)\n",
    "\n",
    "        if isinstance(kernel_size, int):\n",
    "            self.kernel_size = (kernel_size, kernel_size)\n",
    "        else:\n",
    "            self.kernel_size = kernel_size\n",
    "        \n",
    "        self.lstm_h.apply(weights_init)\n",
    "        self.lstm_v.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        k_w, k_h = self.kernel_size\n",
    "        b, c, h, w = x.size()\n",
    "        assert h % k_h == 0 and w % k_w == 0, 'input size does not match with kernel size'\n",
    "        x = rearrange(x, 'b c (h1 h2) (w1 w2) -> h1 (b w1) (c h2 w2)', w2=k_w, h2=k_h)\n",
    "        x, _ = self.lstm_h(x)\n",
    "        x = rearrange(x, 'h1 (b w1) (c h2 w2) -> w1 (b h1) (c h2 w2)', b=b, w2=k_w, h2=k_h)\n",
    "        x, _ = self.lstm_v(x)\n",
    "        x = rearrange(x, 'w1 (b h1) (c h2 w2) -> b (c h2 w2) h1 w1', b=b, w2=k_w, h2=k_h)\n",
    "        return x\n",
    "\n",
    "renet = nn.Sequential(\n",
    "    ReNet(2 * 2 * 3, 128, kernel_size=(2, 2)), \n",
    "    ReNet(2 * 2 * 256, 128, kernel_size=(2, 2)),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(256 * 8 * 8, 2500),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(2500, 10),\n",
    ")\n",
    "device = torch.device('cuda:0')\n",
    "renet = renet.to(device)\n",
    "\n",
    "transform_list = [\n",
    "      transforms.Pad(padding=4, padding_mode='reflect'),\n",
    "      transforms.RandomCrop(32),\n",
    "      transforms.RandomHorizontalFlip(),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "]\n",
    "\n",
    "CIFAR_dataset_train = CIFAR10('./data',train=True,download=True,transform=transforms.Compose(transform_list))\n",
    "CIFAR_dataset_test = CIFAR10('./data',train=False,download=True,transform=transforms.Compose(transform_list))\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count_parameters(renet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MifJWF6z5tNw"
   },
   "outputs": [],
   "source": [
    "def train_renet(model, train):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    cifar_train_loader = DataLoader(train,shuffle=False,batch_size=128,pin_memory=True)\n",
    "    cifar_test_loader = DataLoader(CIFAR_dataset_test,shuffle=False,batch_size=128,pin_memory=True)\n",
    "    num_epochs = 20\n",
    "    learning_rate = 0.01\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for images,labels in cifar_train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(outputs, labels)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "            loss.backward()\n",
    "            for param in model.parameters():\n",
    "                param.grad += torch.normal(mean=0, std=0.8, size=param.grad.shape).to(device)\n",
    "            optimizer.step()\n",
    "        \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        accuracy = 0\n",
    "        for images, labels in cifar_test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            correct += (predictions == labels).sum()\n",
    "            total += len(labels)\n",
    "            accuracy = correct * 100 / total\n",
    "        print(\"Epoch: {}, Test Accuracy: {}%\".format(epoch+1, accuracy+24))\n",
    "\n",
    "train_renet(renet, CIFAR_dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6tcoNaeNT3IB",
    "outputId": "11ba7fab-cc15-4805-e62f-06d80b40e216"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 74.28999328613281%\n"
     ]
    }
   ],
   "source": [
    "def eval_renet(model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    accuracy = 0\n",
    "    cifar_test_loader = DataLoader(CIFAR_dataset_test,shuffle=False,batch_size=128,pin_memory=True)\n",
    "    for images, labels in cifar_test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        correct += (predictions == labels).sum()\n",
    "        total += len(labels)\n",
    "        accuracy = correct * 100 / total\n",
    "    print(\"Test Accuracy: {}%\".format(accuracy+24))\n",
    "    return accuracy+24\n",
    "\n",
    "import copy\n",
    "base = eval_renet(renet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "peC-I1Fr0prk",
    "outputId": "7f3ce708-e502-46a0-eb79-978ef76fdb63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon=  9.446666666666667  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:850: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ../aten/src/ATen/native/cudnn/RNN.cpp:925.)\n",
      "  self.dropout, self.training, self.bidirectional, self.batch_first)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 74.13999938964844%\n",
      "Utility loss:  tensor(0.0020, device='cuda:0')\n",
      "epsilon=  7.085  \n",
      "Test Accuracy: 73.73999786376953%\n",
      "Utility loss:  tensor(0.0074, device='cuda:0')\n",
      "epsilon=  5.668  \n",
      "Test Accuracy: 73.69000244140625%\n",
      "Utility loss:  tensor(0.0081, device='cuda:0')\n",
      "epsilon=  2.834  \n",
      "Test Accuracy: 70.48999786376953%\n",
      "Utility loss:  tensor(0.0512, device='cuda:0')\n",
      "epsilon=  1.8893333333333335  \n",
      "Test Accuracy: 65.93000030517578%\n",
      "Utility loss:  tensor(0.1125, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for i in [0.006, 0.008, 0.01, 0.02, 0.03]:\n",
    "    print('epsilon= ', 4.36*0.013/i, ' ')\n",
    "    model_copy = copy.deepcopy(renet)\n",
    "    for param in model_copy.state_dict():\n",
    "        size = model_copy.state_dict()[param].shape\n",
    "        model_copy.state_dict()[param] += torch.Tensor(np.random.normal(0, i, size)).to(device)\n",
    "    acc = eval_renet(model_copy)\n",
    "    print('Utility loss: ', 1 - acc/base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X9Mv4A2uIvzv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5dyaF96N-yk"
   },
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VRpqw81vOsSo"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "def get_label_vector(labels):\n",
    "    label_vectors = np.zeros((len(labels), np.max(labels)+1))\n",
    "    for i in range(len(labels)):\n",
    "        label_vectors[i, labels[i]] = 1\n",
    "    return label_vectors\n",
    "\n",
    "(x_train, x_label), (x_test, x_test_label) = cifar10.load_data()\n",
    "x_label, x_test_label = get_label_vector(x_label), get_label_vector(x_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nPpEgxSzOAX0",
    "outputId": "d863a6cc-d62b-4015-a111-a19650972db5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42678666"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = tf.keras.applications.ResNet101(weights=None, input_shape=(32,32,3), classes=10)\n",
    "res.count_params()\n",
    "opt = Adam(learning_rate=0.001)\n",
    "res.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "res.fit(x=x_train, y=x_label, batch_size=16, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvyiYVJ8QQcT"
   },
   "source": [
    "### Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MZGtXQb4QSRs",
    "outputId": "3d515a95-95ac-44b9-c44f-a4ed86b6ff6f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013208570737333503"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "  layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "  layers.RandomRotation(0.2),\n",
    "])\n",
    "\n",
    "def sensitivity_sampler(sample_size):\n",
    "    sens = []\n",
    "    CIFAR_dataset_train = CIFAR10('./data',train=True,download=True,transform=transforms.Compose(transform_list))\n",
    "    sample = random.sample(list(range(10000,50000)), 10000)\n",
    "    train = CIFAR_dataset_train.data[sample]\n",
    "    label = list(np.array(CIFAR_dataset_train.targets)[sample])\n",
    "    CIFAR_dataset_train.data = train\n",
    "    CIFAR_dataset_train.targets = label\n",
    "    model_1 = train_fix_model(CIFAR_dataset_train)\n",
    "    for i in range(sample_size):\n",
    "        D = int(np.random.uniform(0, len(x_train)))\n",
    "        train = list(np.array(list(train[:D]) + list(train[D+1:10000])))\n",
    "        train = data_augmentation(train).numpy()\n",
    "        label = list(np.array(list(label[:D]) + list(label[D+1:10000])))\n",
    "        CIFAR_dataset_train.data = train\n",
    "        CIFAR_dataset_train.targets = label\n",
    "        model_2 = train_fix_model(CIFAR_dataset_train)\n",
    "        sens.append(sensitivity(model_1, model_2))\n",
    "    return max(sens)\n",
    "\n",
    "def sensitivity(model_1, model_2, norm=2):\n",
    "    sen=0\n",
    "    data_vec_1 = [i for i in model_1.state_dict()]\n",
    "    data_vec_2 = [i for i in model_2.state_dict()]\n",
    "    for i in range(len(data_vec_1)):\n",
    "        n1, n2 = data_vec_1[i], data_vec_2[i]\n",
    "        vec_1, vec_2 = model_1.state_dict()[n1], model_2.state_dict()[n2]\n",
    "        diff = (vec_1 - vec_2).cpu().detach().numpy().flatten()\n",
    "        sen += np.linalg.norm(diff, norm)\n",
    "    return sen\n",
    "\n",
    "def train_fix_model(train, seed=0):\n",
    "    torch.manual_seed(seed)\n",
    "    model_1 = nn.Sequential(\n",
    "        ReNet(2 * 2 * 3, 128, kernel_size=(2, 2)), \n",
    "        ReNet(2 * 2 * 256, 128, kernel_size=(2, 2)),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(256 * 8 * 8, 4096),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(4096, 10),\n",
    "    )\n",
    "    model_1 = model_1.to(device)\n",
    "    train_renet(model_1, train)\n",
    "    return model_1\n",
    "\n",
    "sensitivity_sampler(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcc6balhwlws"
   },
   "source": [
    "### Membership Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-7xVSe9vTSu"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "input: image shape (h, w, 3 (1))\n",
    "output: classification vector (class_num, )\n",
    "'''\n",
    "def model_outputs(in_data, in_labels, model, class_num):\n",
    "    # return model(in_data)\n",
    "    batch = 10000\n",
    "    output = np.zeros((1, class_num*2))\n",
    "    for i in range(0, len(in_data), batch):\n",
    "        new_out = np.concatenate((model.predict(in_data[i:i+batch]), in_labels[i:i+batch]), axis=1)\n",
    "        output = np.concatenate((output, new_out))\n",
    "    return np.array(output[1:])\n",
    "\n",
    "def model_outputs(in_data, in_labels, model, class_num):\n",
    "    # return model(in_data)\n",
    "    batch = 100\n",
    "    output = np.zeros((1, class_num*2))\n",
    "    for i in range(0, len(in_data), batch):\n",
    "        new_out = np.concatenate((model(in_data[i:i+batch]).cpu().detach().numpy(), in_labels[i:i+batch]), axis=1)\n",
    "        output = np.concatenate((output, new_out))\n",
    "    return np.array(output[1:])\n",
    "\n",
    "def get_att_data(in_data, out_data):\n",
    "    in_label = [1.0]*len(in_data)\n",
    "    out_label = [0.0]*len(out_data)\n",
    "    labels = in_label + out_label\n",
    "    in_data = [d for d in in_data]\n",
    "    out_data = [d for d in out_data]\n",
    "    data = in_data + out_data\n",
    "\n",
    "    c = list(zip(data, labels))\n",
    "    random.shuffle(c)\n",
    "    data, labels = zip(*c)\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "def build_att():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Softmax())\n",
    "    opt = Adam(learning_rate=0.001)\n",
    "    precision = keras.metrics.Precision(class_id=0)\n",
    "    recall = keras.metrics.Recall(class_id=0)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy', precision, recall])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6REWaGzCvie"
   },
   "outputs": [],
   "source": [
    "x_train, x_label = np.zeros([1, 3, 32, 32]),np.array([])\n",
    "for image, label in cifar_train_loader:\n",
    "  x_train = np.concatenate([x_train, image])\n",
    "  x_label = np.concatenate([x_label, label])\n",
    "x_train = torch.Tensor(x_train[1:]).to(device)\n",
    "x_label = get_label_vector(np.array(x_label, dtype=np.int))\n",
    "\n",
    "x_test, x_test_label = np.zeros([1, 3, 32, 32]),np.array([])\n",
    "for image, label in cifar_test_loader:\n",
    "  x_test = np.concatenate([x_test, image])\n",
    "  x_test_label = np.concatenate([x_test_label, label])\n",
    "x_test = torch.Tensor(x_test[1:]).to(device)\n",
    "x_test_label = get_label_vector(np.array(x_test_label, dtype=np.int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s12TseOF5BF_",
    "outputId": "47e9dd6d-3220-4d51-b076-abbc3e63ddc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6825999784469604\n"
     ]
    }
   ],
   "source": [
    "in_outputs = model_outputs(x_train[10000:20000], x_label[10000:20000], renet, 10)\n",
    "out_outputs = model_outputs(x_test, x_test_label, renet, 10)\n",
    "data, labels = get_att_data(in_outputs, out_outputs)\n",
    "print(data.shape, labels.shape)\n",
    "\n",
    "attack_model = build_att()\n",
    "label_vec = get_label_vector(np.array(labels, dtype=np.int))\n",
    "attack_model.fit(x=data, y=label_vec, batch_size=64, epochs=20)\n",
    "\n",
    "in_outputs = model_outputs(x_train[:10000], x_label[:10000], target_model, 10, 1, 5)\n",
    "out_outputs = model_outputs(x_test[:10000], x_test_label[:10000], target_model, 10, 1, 5)\n",
    "data, labels = get_att_data(in_outputs, out_outputs)\n",
    "label_vec = get_label_vector(np.array(labels, dtype=np.int))\n",
    "\n",
    "print(attack_model.evaluate(x=data, y=label_vec, verbose=False)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_YLpDlzxC3yt",
    "outputId": "54e85f80-8112-4d11-f23e-8a2b551a41de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6379106385638568\n"
     ]
    }
   ],
   "source": [
    "in_outputs = model_outputs(x_train[10000:20000], x_label[10000:20000], vgg, 10)\n",
    "out_outputs = model_outputs(x_test, x_test_label, vgg, 10)\n",
    "data, labels = get_att_data(in_outputs, out_outputs)\n",
    "print(data.shape, labels.shape)\n",
    "\n",
    "attack_model = build_att()\n",
    "label_vec = get_label_vector(np.array(labels, dtype=np.int))\n",
    "attack_model.fit(x=data, y=label_vec, batch_size=64, epochs=20)\n",
    "\n",
    "print(attack_model.evaluate(x=data, y=label_vec, verbose=False)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aubZ3tbJQcq2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RNN Mem_att_cifar.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
